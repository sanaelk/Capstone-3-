import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report
from sklearn.metrics import roc_curve, auc

pd.set_option('display.max_columns', None)
plt.style.use('seaborn-v0_8-darkgrid')

# Load the cleaned data
df = pd.read_csv('heart_cleveland_upload.csv')
print("Dataset loaded successfully!")
print(f"Shape: {df.shape}")
print(f"Columns: {df.columns.tolist()}")

# Create a copy of the original data for preprocessing
df_preprocessed = df.copy()

# Separate features and target
X = df_preprocessed.drop('condition', axis=1)
y = df_preprocessed['condition']

# Identify feature types based on EDA
categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
continuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

print("Feature Analysis:")
print(f"Total features: {X.shape[1]}")
print(f"Categorical features: {len(categorical_features)}")
print(f"Continuous features: {len(continuous_features)}")
print(f"Target variable: condition (Binary: 0=No Disease, 1=Disease)")

print("\n=== Creating Dummy Variables ===")
print("-" * 40)

# Check current unique values in categorical features
for feature in categorical_features:
    unique_vals = X[feature].nunique()
    print(f"{feature}: {unique_vals} unique values - {sorted(X[feature].unique())}")

# Create dummy variables for categorical features
# I'll use pd.get_dummies with drop_first=True to avoid dummy variable trap
X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True, dtype=int)

print(f"\nOriginal shape: {X.shape}")
print(f"After encoding shape: {X_encoded.shape}")
print(f"\nNew columns after encoding:")
print(X_encoded.columns.tolist())

# Show sample of encoded data
print("\nSample of encoded features:")
print(X_encoded.head())

print("\n=== Creating Dummy Variables ===")
print("-" * 40)

# Check current unique values in categorical features
for feature in categorical_features:
    unique_vals = X[feature].nunique()
    print(f"{feature}: {unique_vals} unique values - {sorted(X[feature].unique())}")

# Create dummy variables for categorical features
# I'll use pd.get_dummies with drop_first=True to avoid dummy variable trap
X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True, dtype=int)

print(f"\nOriginal shape: {X.shape}")
print(f"After encoding shape: {X_encoded.shape}")
print(f"\nNew columns after encoding:")
print(X_encoded.columns.tolist())

# Show sample of encoded data
print("\nSample of encoded features:")
print(X_encoded.head())

print("\n=== Standardizing Continuous Features ===")
print("-" * 40)

# Before scaling - check ranges
print("Original ranges of continuous features:")
for feature in continuous_features:
    min_val = X[feature].min()
    max_val = X[feature].max()
    mean_val = X[feature].mean()
    std_val = X[feature].std()
    print(f"{feature:10s} Range: [{min_val:6.1f}, {max_val:6.1f}] | "
          f"Mean: {mean_val:6.1f} | Std: {std_val:6.1f}")

# Apply StandardScaler (z-score normalization)
scaler = StandardScaler()
X_scaled = X_encoded.copy()

# Scale only the continuous features
# Note: Dummy variables are already in [0,1] range
scaler.fit(X_scaled[continuous_features])
X_scaled[continuous_features] = scaler.transform(X_scaled[continuous_features])

# Check after scaling
print("\nAfter scaling (StandardScaler):")
for feature in continuous_features:
    min_val = X_scaled[feature].min()
    max_val = X_scaled[feature].max()
    mean_val = X_scaled[feature].mean()
    std_val = X_scaled[feature].std()
    print(f"{feature:10s} Range: [{min_val:6.2f}, {max_val:6.2f}] | "
          f"Mean: {mean_val:6.2f} | Std: {std_val:6.2f}")
    
    print("\n=== Creating Train-Test Split ===")
print("-" * 40)

# Split the data - using 70% train, 30% test
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42, stratify=y
)

print(f"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_scaled)*100:.1f}%)")
print(f"Testing set size: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_scaled)*100:.1f}%)")

# Check class distribution in splits
print("\nClass distribution in splits:")
print("Training set:")
print(y_train.value_counts(normalize=True).round(3))
print("\nTesting set:")
print(y_test.value_counts(normalize=True).round(3))

# Verify stratified sampling worked
train_class_ratio = y_train.value_counts(normalize=True)[1]
test_class_ratio = y_test.value_counts(normalize=True)[1]
original_ratio = y.value_counts(normalize=True)[1]

print(f"\nClass ratio comparison:")
print(f"Original data: {original_ratio:.3f}")
print(f"Training set: {train_class_ratio:.3f}")
print(f"Testing set: {test_class_ratio:.3f}")

print("\n=== Saving Preprocessed Data ===")
print("-" * 40)

# Create a complete preprocessed dataset
df_preprocessed_complete = X_scaled.copy()
df_preprocessed_complete['condition'] = y.values

# Save to CSV
preprocessed_filename = 'heart_disease_preprocessed.csv'
df_preprocessed_complete.to_csv(preprocessed_filename, index=False)
print(f"Preprocessed data saved to: {preprocessed_filename}")

# Save train and test sets separately
train_data = X_train.copy()
train_data['condition'] = y_train.values
train_data.to_csv('heart_disease_train.csv', index=False)

test_data = X_test.copy()
test_data['condition'] = y_test.values
test_data.to_csv('heart_disease_test.csv', index=False)

print("Train and test sets saved separately.")

print("\n" + "="*70)
print("PREPROCESSING SUMMARY")
print("="*70)

print(f"""
1. DATA LOADED:
   • Original shape: {df.shape}
   • Features: {df.shape[1]-1}, Target: 1

2. CATEGORICAL FEATURE ENCODING:
   • Converted {len(categorical_features)} categorical features to dummy variables
   • Original categorical features: {categorical_features}
   • Used drop_first=True to avoid dummy variable trap
   • New feature count: {X_encoded.shape[1]}

3. CONTINUOUS FEATURE SCALING:
   • Scaled {len(continuous_features)} continuous features
   • Features scaled: {continuous_features}
   • Used StandardScaler (z-score normalization)
   • Result: Mean ≈ 0, Std ≈ 1 for all scaled features

4. TRAIN-TEST SPLIT:
   • Split ratio: 70% train, 30% test
   • Random state: 42 (for reproducibility)
   • Stratified by target variable
   • Training samples: {X_train.shape[0]} ({X_train.shape[0]/len(X_scaled)*100:.1f}%)
   • Testing samples: {X_test.shape[0]} ({X_test.shape[0]/len(X_scaled)*100:.1f}%)

5. DATA SAVED:
   • Complete preprocessed dataset: 'heart_disease_preprocessed.csv'
   • Training set: 'heart_disease_train.csv'
   • Testing set: 'heart_disease_test.csv'

DATA IS NOW READY FOR MODELING!
""")
print("="*70)

#Modeling 

print("\n" + "="*70)
print("MODEL DEVELOPMENT")
print("="*70)

# Define evaluation function
def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=""):
    """Evaluate model performance"""
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    y_test_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
    
    # Calculate metrics
    metrics = {
        'train_accuracy': accuracy_score(y_train, y_train_pred),
        'test_accuracy': accuracy_score(y_test, y_test_pred),
        'precision': precision_score(y_test, y_test_pred),
        'recall': recall_score(y_test, y_test_pred),
        'f1': f1_score(y_test, y_test_pred),
        'roc_auc': roc_auc_score(y_test, y_test_prob) if y_test_prob is not None else None
    }
    
    # Print results
    if model_name:
        print(f"\n{model_name} Performance:")
        print("-" * 40)
    
    print(f"Training Accuracy: {metrics['train_accuracy']:.4f}")
    print(f"Testing Accuracy:  {metrics['test_accuracy']:.4f}")
    print(f"Precision:         {metrics['precision']:.4f}")
    print(f"Recall:            {metrics['recall']:.4f}")
    print(f"F1-Score:          {metrics['f1']:.4f}")
    if metrics['roc_auc'] is not None:
        print(f"ROC-AUC:           {metrics['roc_auc']:.4f}")
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_test_pred)
    print(f"\nConfusion Matrix:")
    print(f"[[TN:{cm[0,0]:3d}  FP:{cm[0,1]:3d}]")
    print(f" [FN:{cm[1,0]:3d}  TP:{cm[1,1]:3d}]]")
    
    return model, metrics

# Define function to plot ROC curve
def plot_roc_curve(y_test, y_probs, model_names):
    """Plot ROC curves for multiple models"""
    plt.figure(figsize=(10, 8))
    
    for i, (y_prob, name) in enumerate(zip(y_probs, model_names)):
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)
        
        plt.plot(fpr, tpr, lw=2, 
                label=f'{name} (AUC = {roc_auc:.3f})')
    
    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')
    plt.legend(loc="lower right", fontsize=11)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

    #Baseline Model:Logistic Regression 

    print("\n=== MODEL 1: Logistic Regression (Baseline) ===")
print("-" * 40)

# Initialize and train Logistic Regression
log_reg = LogisticRegression(
    random_state=42,
    max_iter=1000,
    solver='liblinear'
)

log_reg_model, log_reg_metrics = evaluate_model(
    log_reg, X_train, X_test, y_train, y_test, 
    "Logistic Regression"
)

#Random Forest Classifier 

print("\n=== MODEL 2: Random Forest Classifier ===")
print("-" * 40)

# Initialize and train Random Forest
rf_model = RandomForestClassifier(
    random_state=42,
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2
)

rf_model, rf_metrics = evaluate_model(
    rf_model, X_train, X_test, y_train, y_test,
    "Random Forest"
)

# Feature importance analysis for Random Forest
print("\nTop 10 Most Important Features:")
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print(feature_importance.head(10))

# Plot feature importance
plt.figure(figsize=(12, 6))
plt.barh(feature_importance.head(15)['feature'], 
         feature_importance.head(15)['importance'])
plt.xlabel('Importance', fontsize=12)
plt.title('Top 15 Feature Importances - Random Forest', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

#Gradient Boosting Classifier 

print("\n=== MODEL 3: Gradient Boosting Classifier ===")
print("-" * 40)

# Initialize and train Gradient Boosting
gb_model = GradientBoostingClassifier(
    random_state=42,
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3
)

gb_model, gb_metrics = evaluate_model(
    gb_model, X_train, X_test, y_train, y_test,
    "Gradient Boosting"
)

#Model Comparison

print("\n" + "="*70)
print("MODEL COMPARISON")
print("="*70)

# Collect all metrics
models = ['Logistic Regression', 'Random Forest', 'Gradient Boosting']
metrics_list = [log_reg_metrics, rf_metrics, gb_metrics]

# Create comparison dataframe
comparison_df = pd.DataFrame({
    'Model': models,
    'Test Accuracy': [m['test_accuracy'] for m in metrics_list],
    'Precision': [m['precision'] for m in metrics_list],
    'Recall': [m['recall'] for m in metrics_list],
    'F1-Score': [m['f1'] for m in metrics_list],
    'ROC-AUC': [m['roc_auc'] for m in metrics_list]
})

print("\nModel Performance Comparison:")
print("-" * 60)
print(comparison_df.round(4))

# Visual comparison
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
metrics_to_plot = ['Test Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']

for i, metric in enumerate(metrics_to_plot):
    ax = axes[i//3, i%3]
    bars = ax.bar(models, comparison_df[metric], color=['skyblue', 'salmon', 'lightgreen'])
    ax.set_title(f'{metric}', fontsize=12, fontweight='bold')
    ax.set_ylabel('Score', fontsize=10)
    ax.set_ylim([0.7, 1.0])
    ax.grid(axis='y', alpha=0.3)
    
    # Add value labels
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10)

# Remove empty subplot
fig.delaxes(axes[1, 2])
plt.tight_layout()
plt.show()

# Plot ROC curves
print("\nROC Curves Comparison:")
print("-" * 40)

# Get probabilities for ROC curves
y_probs = [
    log_reg_model.predict_proba(X_test)[:, 1],
    rf_model.predict_proba(X_test)[:, 1],
    gb_model.predict_proba(X_test)[:, 1]
]

plot_roc_curve(y_test, y_probs, models)


#Hyperparameter Tuning 

print("\n" + "="*70)
print("HYPERPARAMETER TUNING")
print("="*70)

# Based on initial results, let's tune Random Forest (best performer)
print("\nTuning Random Forest Hyperparameters...")
print("-" * 40)

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# Initialize grid search with cross-validation
rf_tuned = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(
    estimator=rf_tuned,
    param_grid=param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

print("Performing Grid Search (this may take a few minutes)...")
grid_search.fit(X_train, y_train)

print(f"\nBest Parameters: {grid_search.best_params_}")
print(f"Best Cross-Validation Score: {grid_search.best_score_:.4f}")

# Evaluate tuned model
print("\n=== Tuned Random Forest Performance ===")
print("-" * 40)

best_rf = grid_search.best_estimator_
best_rf, best_rf_metrics = evaluate_model(
    best_rf, X_train, X_test, y_train, y_test,
    "Tuned Random Forest"
)

# Compare with original Random Forest
print("\nComparison with Original Random Forest:")
print("-" * 40)
comparison_tuned = pd.DataFrame({
    'Metric': ['Test Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],
    'Original RF': [rf_metrics['test_accuracy'], rf_metrics['precision'], 
                    rf_metrics['recall'], rf_metrics['f1'], rf_metrics['roc_auc']],
    'Tuned RF': [best_rf_metrics['test_accuracy'], best_rf_metrics['precision'], 
                 best_rf_metrics['recall'], best_rf_metrics['f1'], best_rf_metrics['roc_auc']]
})

print(comparison_tuned.round(4))

#Final model selection 

print("\n" + "="*70)
print("FINAL MODEL SELECTION")
print("="*70)

# Select the best model based on F1-Score and ROC-AUC
final_model = best_rf  # Tuned Random Forest
final_metrics = best_rf_metrics

print("\nSelected Final Model: Tuned Random Forest")
print("-" * 40)
print(f"Reasons for selection:")
print("1. Highest F1-Score: {:.4f}".format(final_metrics['f1']))
print("2. High ROC-AUC: {:.4f}".format(final_metrics['roc_auc']))
print("3. Good balance between precision and recall")
print("4. Robust to overfitting (train vs test accuracy similar)")
print("5. Provides feature importance for interpretability")

# Detailed evaluation of final model
print("\n=== Final Model Detailed Evaluation ===")
print("-" * 40)

# Classification report
y_pred_final = final_model.predict(X_test)
print("\nClassification Report:")
print(classification_report(y_test, y_pred_final, target_names=['No Disease', 'Disease']))

# Confusion matrix visualization
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred_final)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Disease', 'Disease'],
            yticklabels=['No Disease', 'Disease'])
plt.title('Confusion Matrix - Final Model', fontsize=14, fontweight='bold')
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.tight_layout()
plt.show()

# Feature importance from final model
print("\nTop 15 Features in Final Model:")
final_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': final_model.feature_importances_
}).sort_values('importance', ascending=False)

print(final_importance.head(15).round(4))

# Plot final feature importance
plt.figure(figsize=(12, 8))
bars = plt.barh(final_importance.head(15)['feature'], 
                final_importance.head(15)['importance'])
plt.xlabel('Importance', fontsize=12)
plt.title('Feature Importance - Final Model (Tuned Random Forest)', 
          fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)

# Add importance values
for bar in bars:
   